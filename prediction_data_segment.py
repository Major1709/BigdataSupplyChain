# predict_with_pytorch.py

import time
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, StringType
from pyspark.sql.functions import from_json, col

# ğŸ§  Spark Session
spark = SparkSession.builder \
    .appName("PyTorchPrediction") \
    .getOrCreate()

# ğŸ“¥ Pipeline ML chargÃ©
pipeline_model = PipelineModel.load("/home/toma/Documents/BIGDATA/big_data_pipeline_v3")
pipeline_seg = PipelineModel.load("/home/toma/Documents/BIGDATA/segmentation_model")

# ğŸ” Boucle continue pour traitement pÃ©riodique
while True:
    print("ğŸ”„ Lecture des nouvelles donnÃ©es...")
    df = spark.read.parquet("hdfs://localhost:9000/projet/output_stream")
    
    if df.isEmpty():
        print("ğŸŸ¡ Aucune donnÃ©e trouvÃ©e. Attente...")
        time.sleep(10)
        continue

    # ğŸ§ª PrÃ©traitement
    df = pipeline_model.transform(df)
    df = pipeline_seg.transform(df)
    

    pandas = df.toPandas()
    pandas_df_clean = pandas.applymap(lambda x: str(x) if x is not None else "")

    # 2. Construire un schÃ©ma explicite (toutes les colonnes en StringType)
    schema = StructType([StructField(col_name, StringType(), True) for col_name in pandas_df_clean.columns])

    # 3. Convertir en liste de lignes (dictionnaires)
    data_list = pandas_df_clean.to_dict(orient="records")

    # 4. CrÃ©er un Spark DataFrame avec schÃ©ma explicite
    spark_df = spark.createDataFrame(data_list, schema=schema)
    # 5. Ã‰crire dans HDFS au format Parquet
    spark_df.write.mode("append").parquet("hdfs://localhost:9000/projet/predictions_segment")
    print("âœ… PrÃ©dictions enregistrÃ©es")
    time.sleep(5)  # Attente avant le prochain traitement
